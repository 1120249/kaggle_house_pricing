# -*- coding: utf-8 -*-
"""log_transform_house_pricing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgP9fOjoUMEldy4Qt9teZMDkL5K_JUzj
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd 

# %matplotlib inline
import matplotlib.pyplot as plt

import pathlib
data_dir = pathlib.Path('/content/drive/MyDrive/ColabNotebooks/house_pricing/data')
df_train = pd.read_csv(data_dir / "train.csv")
df_test = pd.read_csv(data_dir / "test.csv")
df_test_original = pd.read_csv(data_dir / "test.csv")

#print(df_train.describe())

import seaborn as sns
#correlation matrix
corrmat = df_train.corr()
f, ax = plt.subplots(figsize=(20, 15))
sns.heatmap(corrmat, vmax=.8, square=True, annot=True);

#remove variables after analysing heatmap, remove ambiguous variables
vars_to_drop = ['GarageArea','TotRmsAbvGrd','GarageYrBlt','Id']

df_train.drop(vars_to_drop, axis=1, inplace=True)
df_test.drop(vars_to_drop, axis=1, inplace=True)

#Show % of missing values for columns
total = df_train.isnull().sum().sort_values(ascending = False)
percent = (df_train.isnull().sum()/df_train.isnull().count()*100).sort_values(ascending = False)
missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data

"""# Skew variables"""

from scipy.stats import skew
from scipy.stats.stats import pearsonr

#Skew variables - only some variables will suffer log transformation
#after dropping na, only the variables with skew value higher than 0.75 will be stored
#and then they will be log-tranformed

numeric_feats = df_train.dtypes[df_train.dtypes != "object"].index

#calculate skewness for all numeric variables
skewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness

#save only variables with skewness higher than 0.75
skewed_feats = skewed_feats[skewed_feats > 0.75]
skewed_feats = skewed_feats.index
skewed_feats

#log-transform variables (log1p = log(x+ 1)) with skewness > 0.75
df_train[skewed_feats] = np.log1p(df_train[skewed_feats])
vars_to_log = ['MSSubClass', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',
       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',
       'LowQualFinSF', 'GrLivArea', 'BsmtHalfBath', 'KitchenAbvGr',
       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',
       'ScreenPorch', 'PoolArea', 'MiscVal']
df_test[vars_to_log] = np.log1p(df_test[vars_to_log])

"""# Data Cleaning"""

#Replace NA values
from sklearn.impute import SimpleImputer

cols = df_train.columns
nameNumericVariables = df_train._get_numeric_data().columns

nameCatVariables = list(set(cols) - set(nameNumericVariables))
categorical_features_indices = [df_train.columns.get_loc(c) for c in nameCatVariables if c in df_train]

# Clean numerical features with missing values - replace empty values with mean of column
for columnName in nameNumericVariables:
  meanColumn = df_train[columnName].mean() 
  df_train[columnName].fillna(meanColumn, inplace = True)
  if columnName != 'SalePrice':
    df_test[columnName].fillna(meanColumn, inplace = True)


valueReplaceNA = "empty"
# Clean categorical features with missing values - replace with 'empty' string
for columnName in nameCatVariables: 
  df_train[columnName].fillna(valueReplaceNA, inplace = True)
  df_test[columnName].fillna(valueReplaceNA, inplace = True)

print(nameNumericVariables)
print(nameCatVariables)

#Encode categorical labels
#Encode target labels with value between 0 and n_classes-1.
from sklearn.preprocessing import LabelEncoder

#join train and test sets for encoding
all_data = pd.concat([df_train, df_test])

vars_columns={}
for var in nameCatVariables:
  vars_columns[var] = all_data[var].unique()
  encoder = LabelEncoder()
  encoder.fit(vars_columns[var])

  all_data.loc[:, [var]] = encoder.transform(all_data.loc[:, [var]])

#split train and test sets
df_train = all_data[:df_train.shape[0]]
df_test = all_data[df_train.shape[0]:]

#drop target variable - not needed in test set
df_test.drop('SalePrice', axis=1, inplace=True)

all_data

df_test.isnull().sum()

"""# Training"""

import sklearn
from sklearn.model_selection import train_test_split

#train test split
X_train, X_test, y_train, y_test = train_test_split(df_train.drop(['SalePrice'], axis=1), df_train['SalePrice'],
                                                    test_size=0.30, random_state=45)

y_train

y_test

#Lasso algorithm

from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV
from sklearn.model_selection import cross_val_score

def rmse_cv(model):
    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring="neg_mean_squared_error", cv = 5))
    return(rmse)


model_ridge = Lasso()

#plot the RMSE for different values of alpha
alphas = [1, 0.1, 0.001, 0.0005]
cv_ridge = [rmse_cv(Lasso(alpha = alpha)).mean() 
            for alpha in alphas]

cv_ridge = pd.Series(cv_ridge, index = alphas)
cv_ridge.plot(title = "Validation")
plt.xlabel("alpha")
plt.ylabel("rmse")

model_lasso = LassoCV(alphas = [0.1, 0.001, 0.0005]).fit(X_train, y_train)

print('avg rmse ' + str(rmse_cv(model_lasso).mean()))

coef = pd.Series(model_lasso.coef_, index = X_train.columns)

print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  str(sum(coef == 0)) + " variables")

best_features = coef[coef != 0].index
best_features

#Plot coefficients of variables against target variable (house sales price)
#variables with coefficients near 0 do not affect sales price
#if coefficient is negative they negatively affect sales price - reduce the price
#if coefficient is positive they positively affect sales price - increase the price

imp_coef = pd.concat([coef.sort_values()])

plt.rcParams['figure.figsize'] = (25.0, 20.0)
imp_coef.plot(kind = "barh")
plt.title("Coefficients in the Lasso Model")

"""#Tensorflow - Training"""

import tensorflow as tf

from tensorflow import keras
from keras import layers

import math
from keras.regularizers import l1


model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_dim = len(best_features), bias_regularizer=l1(0.001)))


model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_squared_error')


history = model.fit(X_train[best_features], y_train, epochs=25, validation_data=(X_test[best_features], y_test))


print("RMSE")
print(math.sqrt(history.history['val_loss'].pop()))

"""# Testing"""

#Predict with Lasso algorithm on y_test (validation set)
y_predictions = model_lasso.predict(X_test)


from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_log_error

err = mean_squared_error(y_test, y_predictions)
print(math.sqrt(err))

#rmsle = mean_squared_log_error(y_dt_test, y_predictions)
#print(math.sqrt(rmsle))

#Plot predicted values against real values to show differences
a = plt.axes(aspect='equal')
plt.rcParams['figure.figsize'] = (8.0, 10.0)
plt.scatter(y_test, y_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [10.5, 13]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

#predict with test.csv - df_test has the test dataset, where the target values are not known
y_predictions = model_lasso.predict(df_test)

"""# Export"""

predictions_test_dataset = model_lasso.predict(df_test)
predictions_test_dataset = np.expm1(predictions_test_dataset)
df_export = pd.DataFrame({"Id":df_test_original["Id"], "SalePrice":predictions_test_dataset})
df_export.to_csv('result.csv', index=False)